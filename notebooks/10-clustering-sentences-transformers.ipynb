{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import string\n",
    "\n",
    "from ipywidgets import interact\n",
    "\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "import umap\n",
    "import scipy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy_fi_experimental_web_md\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise\n",
    "from sklearn.preprocessing import normalize\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "from utils.preprocessing import preprocess_func\n",
    "import utils.preprocessing as preproc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_stop = ['jne.', 'em.', 'esim.', 'tms.', 'mm.', 'yms.', 'redacted', 'pitää', 'http', 'voida', 'haluta', 'syventää', 'esimerkki', 'taito', 'kiinnostaa', 'mennä', 'meno', 'estää', 'kehittää', 'kehittäminen', 'erityisesti', 'onneksi', 'tämä', 'näkyä', 'käyttö', 'osata', 'kehittää', 'työ', 'taito', 'kehittyä', 'oppia', 'liittyvä', 'osaaminen', 'käyttö', 'lisätä', 'haluta']\n",
    "\n",
    "for w in _stop:\n",
    "    if w.endswith('.'):\n",
    "        _stop.append(w[:-1])\n",
    "\n",
    "STOP = set(stopwords.words('finnish') \n",
    "           + open('data/external/stopwords.txt').read().splitlines()\n",
    "           + _stop\n",
    "          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = preprocess_func(lemmatize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy_fi_experimental_web_md.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tokenizer_exception_s = \"\"\"\n",
    "esim. - esimerkiksi\n",
    "Esim. - Esimerkiksi\n",
    "ym. - ynnä muuta\n",
    "tms. - tai muuta sellaista\n",
    "jne. - ja niin edelleen\n",
    "kts. - katso\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_custom_token_exceptions(s):\n",
    "    \n",
    "    custom_token_exceptions = []\n",
    "\n",
    "    for exception in custom_tokenizer_exception_s.split('\\n'):\n",
    "        if not exception:\n",
    "            # skip blank lines\n",
    "            continue\n",
    "\n",
    "        parts = exception.split('-')\n",
    "\n",
    "        s = parts[0].strip()\n",
    "        substrings = parts[1].strip().split()\n",
    "\n",
    "        custom_token_exceptions.append((s, [{'ORTH': s}]))\n",
    "        \n",
    "    return custom_token_exceptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exceptions = get_custom_token_exceptions(custom_tokenizer_exception_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s, substrings in exceptions:\n",
    "    nlp.tokenizer.add_special_case(s, substrings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('data/processed/ensisijainen.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame({'sentence': [sent.text.strip() for para in open('lorem.txt', 'r').read().split('\\n') if para for sent in nlp(para).sents]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['organisaatio1'].dropna().unique())\n",
    "\n",
    "# org1 = 'Sosiaali- ja terveystoimiala'\n",
    "org1 = 'Kaupunginkanslia'\n",
    "# \n",
    "df = df[df['organisaatio1'] == org1].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentences'] = df['answer']\\\n",
    ".str.replace('\\[redacted\\]', 'REDACTED')\\\n",
    ".apply(lambda text: [line for line in text.split('\\n') if line.strip()])\\\n",
    ".apply(lambda lines: [sent.text.strip() for line in lines for sent in nlp(line).sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = pd.DataFrame([{'sentence': sent, 'doc_idx': t.Index} for t in df.itertuples() for sent in t.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('xlm-r-distilroberta-base-paraphrase-v1')\n",
    "# model = SentenceTransformer('LaBSE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(dd['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale to unit length\n",
    "normalized_emb = normalize(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 20\n",
    "\n",
    "# clusterer = hdbscan.HDBSCAN()\n",
    "clusterer = KMeans(n_clusters=n_clusters)\n",
    "labels = clusterer.fit_predict(normalized_emb)\n",
    "dd['cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(dd['cluster']).hist(bins=len(dd['cluster'].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 400)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['lemmatized_sentence'] = dd['sentence'].apply(preprocess_func(lemmatize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_document_df = dd.groupby('cluster').agg({'lemmatized_sentence': lambda s: ' '.join(s)})\n",
    "\n",
    "cluster_vec = TfidfVectorizer()\n",
    "\n",
    "cluster_tfidf_weights = cluster_vec.fit_transform(cluster_document_df['lemmatized_sentence'])\n",
    "\n",
    "tfidf_words = [word for word, idx in sorted(cluster_vec.vocabulary_.items(), key=lambda t: t[1])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cluster_i in dd['cluster'].unique():\n",
    "    tfidf_row_weights = cluster_tfidf_weights[cluster_i].toarray().squeeze()\n",
    "    top_words_and_scores = sorted(zip(tfidf_words, tfidf_row_weights), key=lambda t: t[1], reverse=True)\n",
    "    cluster_document_df.loc[cluster_i, 'top_words'] = ' '.join([w for w, _ in top_words_and_scores if w not in STOP][:10])\n",
    "#     cluster_document_df.loc[cluster_i, 'avg length'] = sent_df.loc[sent_df['cluster'] == cluster_i, 'sentence'].apply(lambda s: len(s.split())).mean()\n",
    "    cluster_document_df.loc[cluster_i, 'n'] = (dd['cluster'] == cluster_i).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_document_df.drop(columns=['lemmatized_sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact(cluster=dd['cluster'].sort_values().unique())\n",
    "def _f(cluster):\n",
    "    display(dd.loc[dd['cluster'] == cluster, ['sentence']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'apotti-osaaminen ja kirjaaminen'\n",
    "\n",
    "v = normalize(model.encode([query])).reshape(-1)\n",
    "\n",
    "scores = normalized_emb @ v\n",
    "\n",
    "highest_score_idx = scores.argsort()[-20:][::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores[highest_score_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd['sentence'].iloc[highest_score_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.to_csv('lorem.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

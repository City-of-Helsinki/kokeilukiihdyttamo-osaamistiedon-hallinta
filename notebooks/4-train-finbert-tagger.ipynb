{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade tensorflow-gpu==2.2 -q # must be at least 2.2 to use transformers\n",
    "# !pip install pytorch_lightning -q\n",
    "# !pip install transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import ArgumentParser\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import string\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import *\n",
    "\n",
    "from ast import literal_eval\n",
    "\n",
    "from transformers import AutoModelForTokenClassification, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('TurkuNLP/bert-base-finnish-uncased-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('row_df_fi_labeled.csv', index_col=0)\n",
    "\n",
    "# select rows which have labels\n",
    "train_df = df[~df['label'].isna()].copy()\n",
    "\n",
    "# rest is test\n",
    "test_df = df.drop(index=train_df.index)\n",
    "\n",
    "# convert serialized lists back into lists objects\n",
    "train_df['label'] = train_df['label'].apply(literal_eval)\n",
    "\n",
    "len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2label = dict(enumerate('OBI'))\n",
    "\n",
    "label2int = {v: k for k, v in int2label.items()}\n",
    "\n",
    "label2int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_labels = np.array([tag for label in train_df['label'] for tag in label])\n",
    "\n",
    "class_weights = class_weight.compute_class_weight(class_weight='balanced', classes=list('OBI'), y=all_labels)\n",
    "\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans({c: f' {c} ' for c in string.punctuation})\n",
    "\n",
    "def tokenize(s):\n",
    "    return s.translate(table).strip().split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ds(torch.utils.data.Dataset):\n",
    "    def __init__(self, tokens, labels=None):\n",
    "        self.tokens = tokens\n",
    "        self.labels = labels\n",
    "        \n",
    "    def __getitem__(self, i):\n",
    "        item = {'input_ids': self.tokens[i]}\n",
    "        \n",
    "        if self.labels is not None:\n",
    "            item['labels'] = [label2int[lbl] for lbl in self.labels[i]]\n",
    "                \n",
    "        return item\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(df, train=True):\n",
    "\n",
    "    data_tokens = []\n",
    "    data_labels = []\n",
    "\n",
    "    for t in df.itertuples():\n",
    "        text = t.answer\n",
    "        word_labels = t.label\n",
    "        \n",
    "        if train:\n",
    "\n",
    "            # get tokens using a simple whitespace tokenizer\n",
    "            word_tokens = tokenize(text)\n",
    "\n",
    "            # add O to labels and the corresponding [CLS] token to tokens\n",
    "            labels = ['O']\n",
    "            tokens = [tokenizer.cls_token_id]\n",
    "\n",
    "            # use model tokenizer to repeat BIO tokens\n",
    "            for word_token, word_label in zip(word_tokens, word_labels):\n",
    "                word_tokens = tokenizer.encode(word_token, add_special_tokens=False)\n",
    "\n",
    "                labels.extend(len(word_tokens) * word_label)\n",
    "                tokens.extend(word_tokens)\n",
    "\n",
    "            # add O and [SEP] tokens\n",
    "            labels.append('O')\n",
    "            tokens.append(tokenizer.sep_token_id)\n",
    "\n",
    "            # tokenizing whitespace separated text should be the same as tokenizing full documents\n",
    "            assert tokens == tokenizer.encode(text)\n",
    "\n",
    "            # token and label sequences should have equal lengths\n",
    "            assert len(labels) == len(tokens)\n",
    "\n",
    "            data_tokens.append(tokens)\n",
    "            data_labels.append(labels)    \n",
    "            \n",
    "        else:\n",
    "            tokens = tokenizer.encode(text)\n",
    "            data_tokens.append(tokens)\n",
    "        \n",
    "        \n",
    "    return Ds(data_tokens, data_labels if train else None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = create_dataset(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples):\n",
    "    batch = tokenizer.pad(examples, padding='longest')\n",
    "    \n",
    "    if 'labels' in batch:\n",
    "\n",
    "        # for some reason tokenizer.pad does not pad `labels` so let's do it manually\n",
    "        label_sequences = [torch.tensor(x) for x in batch['labels']]\n",
    "        batch['labels'] = torch.nn.utils.rnn.pad_sequence(label_sequences, batch_first=True, padding_value=-1)\n",
    "\n",
    "    # we cannot pass return_tensors='pt' because stacking `labels` would fail\n",
    "    # so let's also do this conversion manually\n",
    "    batch['input_ids'] = torch.tensor(batch['input_ids'])\n",
    "    batch['attention_mask'] = torch.tensor(batch['attention_mask'])    \n",
    "    \n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batch = collate_fn([train_ds[i] for i in range(8)])\n",
    "\n",
    "test_config = AutoConfig.from_pretrained('TurkuNLP/bert-base-finnish-uncased-v1', num_labels=3)\n",
    "test_model = AutoModelForTokenClassification.from_config(test_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = test_model(**test_batch)\n",
    "out[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self, lr, batch_size, **kwargs):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        config = AutoConfig.from_pretrained('TurkuNLP/bert-base-finnish-uncased-v1', num_labels=3)\n",
    "        self.model = AutoModelForTokenClassification.from_config(config)\n",
    "\n",
    "    def forward(self, *args, **kwargs):\n",
    "        return self.model(*args, **kwargs)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         loss, logits = self(**batch)\n",
    "#         print('sums:', logits.argmax(-1).sum(dim=[0, 1]))\n",
    "#         print('loss:', loss)\n",
    "#         return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \n",
    "        assert (batch['labels'] != -1).sum() == batch['attention_mask'].sum()\n",
    "\n",
    "        _, logits = self(**batch)\n",
    "        \n",
    "        attention_mask = batch['attention_mask']\n",
    "        labels = batch['labels']\n",
    "\n",
    "\n",
    "\n",
    "        # NOTE: this is noral transformers BERT loss which has been modified by adding class weights\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            loss_fct = torch.nn.CrossEntropyLoss(weight=torch.tensor(class_weights, dtype=torch.float, device=logits.device))\n",
    "\n",
    "            # Only keep active parts of the loss\n",
    "            if attention_mask is not None:\n",
    "                active_loss = attention_mask.view(-1) == 1\n",
    "                active_logits = logits.view(-1, self.model.num_labels)\n",
    "                active_labels = torch.where(\n",
    "                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n",
    "                )\n",
    "                loss = loss_fct(active_logits, active_labels)\n",
    "            else:\n",
    "                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))        \n",
    "                \n",
    "        print(f'loss {self.global_step}:', loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.AdamW(self.parameters(), lr=self.hparams['lr'])\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(train_ds, \n",
    "                          batch_size=args.batch_size, \n",
    "                          shuffle=True,\n",
    "                          num_workers=4,\n",
    "                          drop_last=True,\n",
    "                          collate_fn=collate_fn)\n",
    "\n",
    "    @staticmethod\n",
    "    def add_model_specific_args(parent_parser):\n",
    "        parser = ArgumentParser(parents=[parent_parser], add_help=False)\n",
    "        parser.add_argument('--lr', type=float, default=1e-4)\n",
    "        parser.add_argument('--batch_size', default=16, type=int)\n",
    "        return parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pl.seed_everything(1234)\n",
    "\n",
    "# ------------\n",
    "# args\n",
    "# ------------\n",
    "parser = ArgumentParser()\n",
    "parser = pl.Trainer.add_argparse_args(parser)\n",
    "parser = Model.add_model_specific_args(parser)\n",
    "args = parser.parse_args([])\n",
    "\n",
    "# ------------\n",
    "# model\n",
    "# ------------\n",
    "model = Model(**vars(args))\n",
    "\n",
    "# ------------\n",
    "# training\n",
    "# ------------\n",
    "epochs = 3\n",
    "\n",
    "trainer = pl.Trainer(logger=False,\n",
    "                     min_epochs=epochs,\n",
    "                     max_epochs=epochs,\n",
    "                     gpus=0)\n",
    "trainer.fit(model)\n",
    "\n",
    "# ------------\n",
    "# testing\n",
    "# ------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = create_dataset(test_df, train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_ds = TestDataset(test_list)\n",
    "\n",
    "# dl = DataLoader(test_ds, batch_size=32, collate_fn=test_collate)\n",
    "\n",
    "dl = DataLoader(test_ds, batch_size=32, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style = \"\"\"\n",
    "<style>\n",
    "\n",
    ".B {\n",
    "    color: orange\n",
    "}\n",
    "\n",
    ".I {\n",
    "    color: blue\n",
    "}\n",
    "\n",
    ".O {\n",
    "    color: black\n",
    "}\n",
    "\n",
    "</style>\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmls = []\n",
    "\n",
    "# for test_batch in model.train_dataloader():\n",
    "for test_batch in dl:\n",
    "    pred = model(input_ids=test_batch['input_ids'], \n",
    "                 attention_mask=test_batch['attention_mask'])[0].argmax(-1)\n",
    "\n",
    "    for i in range(len(pred)):\n",
    "        x = tokenizer.convert_ids_to_tokens(test_batch['input_ids'][i])\n",
    "        y = [int2label[bio_int.item()] for bio_int in pred[i]]\n",
    "        \n",
    "        pred_tokens = []\n",
    "\n",
    "        for x_, y_ in zip(x, y):\n",
    "            if x_ in ['[CLS]', '[SEP]', '[PAD]']:\n",
    "                continue\n",
    "            \n",
    "            w = f'<span class=\"{y_}\">{x_}</span>'\n",
    "            \n",
    "            pred_tokens.append(w)\n",
    "\n",
    "        htmls.append(HTML(style + ' '.join(pred_tokens)))\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VBox(htmls)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
